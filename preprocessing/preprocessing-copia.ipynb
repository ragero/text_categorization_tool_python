{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/rafael/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/rafael/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/rafael/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to /home/rafael/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Libraries to manage text data \n",
    "\n",
    "## SKLearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "## NLTK\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('rslp')\n",
    "stemmer_pt = RSLPStemmer()\n",
    "stemmer_en = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "## Python\n",
    "import string\n",
    "\n",
    "## Gensim\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "#Libraries to manage the file system\n",
    "import os\n",
    "\n",
    "#Other libraries\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import scipy\n",
    "import joblib\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_pt = set({})\n",
    "stopwords_en = set({})\n",
    "path_stop_pt = './stopPort.txt'\n",
    "path_stop_en = './stopIngl.txt'\n",
    "\n",
    "if(os.path.exists(path_stop_pt) and os.path.exists(path_stop_en)): \n",
    "    with open(path_stop_pt) as file_stop_pt:\n",
    "        for line in file_stop_pt.readlines():\n",
    "            stopwords_pt.add(line.strip())\n",
    "    with open(path_stop_en) as file_stop_en:\n",
    "        for line in file_stop_en.readlines():\n",
    "            stopwords_en.add(line.strip())\n",
    "else: \n",
    "    stopwords_pt = set(stopwords.words('portuguese'))\n",
    "    stopwords_en = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class to Tokenize and Clean the Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor(object): \n",
    "    \n",
    "    def __init__(self, language='en', remove_stopwords=True, remove_punctuation=True, \n",
    "                 convert_numbers = True, remove_numbers = False, simplification=True, \n",
    "                 simplification_type='lemmatization', lower_case = True): \n",
    "        self.language = language\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.convert_numbers = convert_numbers\n",
    "        self.remove_numbers = remove_numbers\n",
    "        self.simplification = simplification\n",
    "        self.simplification_type = simplification_type \n",
    "        self.lower_case = lower_case\n",
    "\n",
    "\n",
    "    # Complete function to standardize the text\n",
    "    def text_cleaner(self, text): \n",
    "        new_text = ''\n",
    "        stopwords = None \n",
    "\n",
    "        if self.language == 'en':\n",
    "            stopwords = stopwords_en \n",
    "        else:\n",
    "            stopwords = stopwords_pt\n",
    "\n",
    "        if self.lower_case == True: \n",
    "            text = text.lower()\n",
    "\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        if self.remove_stopwords == True:\n",
    "            new_tokens = []\n",
    "            for token in tokens: \n",
    "                if token in stopwords:\n",
    "                    continue \n",
    "                else: \n",
    "                    new_tokens.append(token)\n",
    "            tokens = new_tokens \n",
    "\n",
    "        if self.remove_punctuation == True: \n",
    "            new_tokens = []\n",
    "            for token in tokens: \n",
    "                if token in string.punctuation:\n",
    "                    continue \n",
    "                else: \n",
    "                    new_tokens.append(token)\n",
    "            tokens = new_tokens \n",
    "        \n",
    "        if self.remove_numbers == True:\n",
    "            new_tokens = []\n",
    "            for token in tokens: \n",
    "                if token.isnumeric():\n",
    "                    continue\n",
    "                new_tokens.append(token)\n",
    "            tokens = new_tokens \n",
    "        \n",
    "        if self.convert_numbers == True: \n",
    "            new_tokens = []\n",
    "            for token in tokens: \n",
    "                if token.isnumeric():\n",
    "                    new_tokens.append(\"0\"*len(token))\n",
    "                else: \n",
    "                    new_tokens.append(token)\n",
    "            tokens = new_tokens \n",
    "\n",
    "        if self.simplification == True: \n",
    "            new_tokens = []\n",
    "            if self.language == 'en': \n",
    "                if self.simplification_type  == 'lemmatization':\n",
    "                    for token in tokens: \n",
    "                        new_tokens.append(lemmatizer.lemmatize(token))\n",
    "                elif self.simplification_type  == 'stemming':\n",
    "                    for token in tokens: \n",
    "                        new_tokens.append(stemmer_en.stem(token))\n",
    "                else: \n",
    "                    raise ValueError('Unsuported language. Please, use language = {\"pt\",\"en\"}.')\n",
    "            elif self.language == 'pt':\n",
    "                for token in tokens: \n",
    "                        new_tokens.append(stemmer_en.stem(token))\n",
    "            else: \n",
    "                raise ValueError('Unsuported language. Please, use language = {\"pt\",\"en\"}.')\n",
    "            tokens = new_tokens\n",
    "\n",
    "        return ' '.join(tokens).strip()\n",
    "\n",
    "\n",
    "    #Just a simple tokenizer\n",
    "    def tokenizer(self, text):\n",
    "        text = text.lower()\n",
    "        lista_alfanumerica = []\n",
    "\n",
    "        for token in nltk.word_tokenize(text):\n",
    "            if token in string.punctuation:\n",
    "                continue \n",
    "            if token in stopwords_en: \n",
    "                continue\n",
    "            if token.isnumeric():\n",
    "                token = \"0\"*len(token)\n",
    "\n",
    "            lista_alfanumerica.append(token)\n",
    "        return lista_alfanumerica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Save and Load the Presentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_representation(representation, path): \n",
    "        joblib.dump(representation,path)\n",
    "        \n",
    "def load_representation(path): \n",
    "    return joblib.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class to Generate a Standard Representation for Different Space Vector Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructuredRepresentation():\n",
    "\n",
    "    def __init__(self, doc_vectors=None, class_vectors=None, vocabulary=None): \n",
    "        self.text_vectors = doc_vectors\n",
    "        self.class_vectors = class_vectors \n",
    "        self.vocabulary = vocabulary\n",
    "\n",
    "  \n",
    "    def save_arff(self, name, path, non_sparse_format = False):\n",
    "        num_docs = self.text_vectors.shape[0]\n",
    "        num_attrs = self.text_vectors.shape[1]\n",
    "        with open(path, 'w') as arff: \n",
    "            #Writting the relation\n",
    "            arff.write(f'@relation {name}\\n\\n')\n",
    "            \n",
    "            #Writting the attributes\n",
    "            if self.vocabulary == None: \n",
    "                for attr in range(num_attrs): \n",
    "                    arff.write(f'@ATTRIBUTE dim{attr + 1} NUMERIC\\n')\n",
    "            else: \n",
    "                sorted_vocabulary = sorted(self.vocabulary.items(), key=lambda x: x[1])\n",
    "                for attr in range(num_attrs): \n",
    "                    arff.write(f'@ATTRIBUTE {sorted_vocabulary[attr][0]} NUMERIC\\n')\n",
    "            \n",
    "            #Writting the class names\n",
    "            arff.write('@ATTRIBUTE att_class ' + '{\"' + '\",\"'.join(self.class_vectors.unique()) + '\"}\\n\\n')\n",
    "\n",
    "\n",
    "            #Writting the data\n",
    "            arff.write('@data\\n\\n')\n",
    "\n",
    "            if non_sparse_format == False: \n",
    "                for doc in range(num_docs):\n",
    "                    vector = self.text_vectors[doc]\n",
    "                    if type(vector) == scipy.sparse.csr.csr_matrix: \n",
    "                        vector = self.text_vectors[doc].toarray()[0]\n",
    "                    str_vec = ''\n",
    "                    for i in range(vector.shape[0]): \n",
    "                        str_vec += str(vector[i]) + ','\n",
    "                    classe = self.class_vectors.iloc[doc]\n",
    "                    arff.write(str_vec + '\"' + classe + '\"\\n') \n",
    "            else: \n",
    "                for doc in range(num_docs):\n",
    "                    vector = self.text_vectors[doc]\n",
    "                    if type(vector) == scipy.sparse.csr.csr_matrix: \n",
    "                        vector = self.text_vectors[doc].toarray()[0]\n",
    "                    str_vec = ''\n",
    "                    for i in range(vector.shape[0]): \n",
    "                        if vector[i] > 0: \n",
    "                            str_vec += f'{i} {str(vector[i])},'\n",
    "                    classe = self.class_vectors.iloc[doc]\n",
    "                    arff.write('{' + str_vec + str(num_attrs) + ' \"' + classe + '\"}\\n') \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes to Generate Vector Space Model Based Representaions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words or Bag-of-N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySparseVSM: \n",
    "\n",
    "    def __init__(self, weight='tf', n_grams=1):\n",
    "        self.vectorizer = None \n",
    "        if(weight == 'tf'):\n",
    "            self.vectorizer = CountVectorizer(min_df=2, ngram_range=(1, n_grams), dtype=np.uint8)\n",
    "        else:\n",
    "            self.vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, n_grams), dtype=np.uint8)\n",
    "\n",
    "        self.structured_representation = None\n",
    "\n",
    "    def build_representation(self, texts, classes): \n",
    "        self.structured_representation = StructuredRepresentation(self.vectorizer.fit_transform(texts), classes, self.vectorizer.vocabulary_)\n",
    "        return self.structured_representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low Dimensional Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SuperClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowDimensionalRepresentation(object):\n",
    "    \n",
    "    def __init__(self, dim_size = 200, model = None, num_threads=1, min_count = 2, window_size = 5, num_max_epochs= 100, alpha = 0.025, min_alpha = 0.0001): \n",
    "        __metaclass__  = ABCMeta\n",
    "        self.dim_size = dim_size\n",
    "        self.model = model \n",
    "        self.num_threads = num_threads \n",
    "        self.min_count = min_count\n",
    "        self.window_size = window_size\n",
    "        self.num_max_epochs = num_max_epochs\n",
    "        self.alpha = alpha\n",
    "        self.min_alpha = min_alpha\n",
    "\n",
    "    @abstractmethod\n",
    "    def build_representation(self, texts, classes): \n",
    "        pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyWord2Vec (LowDimensionalRepresentation):\n",
    "    \n",
    "    def __init__(self, dim_size = 200, model = 'skip-gram', method='average', num_threads=1, min_count = 2, window_size = 5, num_max_epochs = 100, alpha = 0.025, min_alpha = 0.0001): \n",
    "        super(MyWord2Vec,self).__init__(dim_size,model,num_threads,min_count,window_size, num_max_epochs, alpha, min_alpha)\n",
    "        self.language_model = None \n",
    "        self.cg = None \n",
    "        if method != 'average' and method != 'sum': \n",
    "            raise ValueError('Unsuported method. Please, use method = {\"average\",\"sum\"}.')\n",
    "        self.method = method\n",
    "        \n",
    "\n",
    "    def build_model(self, texts):\n",
    "        language_model = None\n",
    "        sg = 0\n",
    "        if self.model == 'cbow' : \n",
    "            language_model = gensim.models.Word2Vec\n",
    "        elif self.model == 'sg': \n",
    "            language_model = gensim.models.Word2Vec\n",
    "            sg = 1\n",
    "        #elif self.model == 'glove': \n",
    "            #self.language_model = gensim.models.Word2Vec(list_tokens_texts,min_count=min_count,window=window_size, size=dim_size, workers=num_threads)\n",
    "        elif self.model == 'fasttext': \n",
    "            language_model = gensim.models.FastText\n",
    "            sg = 1\n",
    "        else: \n",
    "            raise ValueError('Unsuported language model. Please, use language model = {\"cbow\",\"sg\",\"fasttext\"}.')\n",
    "\n",
    "        list_tokens_texts = texts.apply(self.tokenizer)\n",
    "        self.language_model = language_model(list_tokens_texts,sg=sg, min_count=self.min_count,\n",
    "                                             window=self.window_size, size=self.dim_size, workers=self.num_threads, \n",
    "                                             iter=self.num_max_epochs, alpha = self.alpha, min_alpha = self.min_alpha)\n",
    "\n",
    "    def build_representation(self, texts, classes): \n",
    "        self.build_model(texts)\n",
    "        matrix = np.zeros((len(texts),self.dim_size))\n",
    "\n",
    "        for i in range(len(texts)):\n",
    "            tokens = self.tokenizer(texts.iloc[i])\n",
    "            matrix[i] = self.sum_vectors(tokens)\n",
    "            if(self.method == 'average' and len(tokens) > 0): \n",
    "                matrix[i] = matrix[i]/len(tokens)\n",
    "\n",
    "\n",
    "        self.structured_representation = StructuredRepresentation(matrix, classes, None)\n",
    "        return self.structured_representation\n",
    "\n",
    "    def tokenizer(self,text):\n",
    "        text = text.lower()\n",
    "        lista_alfanumerica = []\n",
    "\n",
    "        for token in nltk.word_tokenize(text):\n",
    "            if token in string.punctuation:\n",
    "                continue \n",
    "            if token in stopwords_en: \n",
    "                continue\n",
    "            if token.isnumeric():\n",
    "                token = \"0\"*len(token)\n",
    "\n",
    "            lista_alfanumerica.append(token)\n",
    "        return lista_alfanumerica\n",
    "\n",
    "    def sum_vectors(self,lista_tokens): \n",
    "        vetor_combinado = np.zeros(self.dim_size)\n",
    "        for token in lista_tokens: \n",
    "            try:\n",
    "                vetor_combinado += self.language_model.wv.get_vector(token)\n",
    "            except KeyError:\n",
    "                continue \n",
    "        return vetor_combinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDoc2Vec (LowDimensionalRepresentation):\n",
    "    \n",
    "    def __init__(self, dim_size = 200, model = 'dm', method='average', num_threads=4, alpha = 0.025, min_alpha=0.0001, num_max_epochs = 2000,min_count = 1, window_size = 5): \n",
    "        super(MyDoc2Vec,self).__init__(dim_size,model,num_threads,min_count,window_size, num_max_epochs, alpha, min_alpha)\n",
    "        self.model = model\n",
    "      \n",
    "\n",
    "        self.dm = -1\n",
    "        if model == 'dbow':\n",
    "            self.dm = 0\n",
    "        elif model == 'dm':\n",
    "            self.dm = 1\n",
    "        elif model != 'both':\n",
    "            raise ValueError('Unsuported model. Please, use model = {\"dm\",\"dbow\"}.')\n",
    "        \n",
    "        self.dm_mean = 1\n",
    "        if method == 'average': \n",
    "            self.dm_concat = 0\n",
    "        elif method == 'concat':\n",
    "            self.dm_concat = 1\n",
    "        else:\n",
    "            raise ValueError('Unsuported method. Please, use method = {\"concat\",\"average\"}.')\n",
    "        \n",
    "        #standard parameters\n",
    "        self.hs = 0\n",
    "        self.dbow_words = 0\n",
    "        \n",
    "    \n",
    "    def build_model(self, texts): \n",
    "        \n",
    "        tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(texts)]\n",
    "        if self.model == 'dm' or self.model == 'dbow': \n",
    "            model = Doc2Vec(vector_size=self.dim_size, alpha=self.alpha, min_alpha=self.min_alpha, \n",
    "                            min_count=self.min_count, dm=self.dm, workers = self.num_threads,\n",
    "                            dm_min = self.dm_mean, dm_concat = self.dm_concat,\n",
    "                            dbow_words = self.dbow_words, hs=self.hs, epochs=self.num_max_epochs, seed=1)\n",
    "            model.build_vocab(tagged_data)\n",
    "            model.train(tagged_data,total_examples=model.corpus_count,epochs=model.iter)\n",
    "            \n",
    "            #Reduce memory usage\n",
    "            model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "            matrix = np.zeros((len(texts),self.dim_size))\n",
    "            for i in range(model.corpus_count): \n",
    "                matrix[i] = model.docvecs[str(i)]\n",
    "            return matrix\n",
    "        elif self.model == 'both': \n",
    "            modelDM = Doc2Vec(vector_size=self.dim_size, alpha=self.alpha, min_alpha=self.min_alpha, \n",
    "                            min_count=self.min_count, dm=1, workers = self.num_threads,\n",
    "                            dm_min = self.dm_mean, dm_concat = self.dm_concat,\n",
    "                            dbow_words = self.dbow_words, hs=self.hs, epochs=self.num_max_epochs, seed=1)\n",
    "            modelDBOW = Doc2Vec(vector_size=self.dim_size, alpha=self.alpha, min_alpha=self.min_alpha, \n",
    "                            min_count=self.min_count, dm=0, workers = self.num_threads,\n",
    "                            dm_min = self.dm_mean, dm_concat = self.dm_concat,\n",
    "                            dbow_words = self.dbow_words, hs=self.hs, epochs=self.num_max_epochs, seed=1)\n",
    "                        \n",
    "            modelDM.build_vocab(tagged_data)\n",
    "            modelDBOW.build_vocab(tagged_data)\n",
    "\n",
    "            modelDM.train(tagged_data,total_examples=modelDM.corpus_count,epochs=modelDM.iter)\n",
    "            modelDBOW.train(tagged_data,total_examples=modelDBOW.corpus_count,epochs=modelDBOW.iter)\n",
    "            \n",
    "            #Reduce memory usage\n",
    "            modelDM.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "            modelDBOW.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "\n",
    "            matrixDM = np.zeros((len(texts),self.dim_size))\n",
    "            for i in range(modelDM.corpus_count): \n",
    "                matrixDM[i] = modelDM.docvecs[str(i)]\n",
    "            matrixDBOW = np.zeros((len(texts),self.dim_size))\n",
    "            for i in range(modelDBOW.corpus_count): \n",
    "                matrixDBOW[i] = modelDBOW.docvecs[str(i)]\n",
    "            \n",
    "            matrix = np.concatenate([matrixDM, matrixDBOW], axis=1)\n",
    "            return matrix\n",
    "\n",
    "  \n",
    "\n",
    "    def build_representation(self, texts, classes): \n",
    "        self.structured_representation = StructuredRepresentation(self.build_model(texts), classes, None)\n",
    "        return self.structured_representation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Área de Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um dicionario (versão completa)\n",
    "config = {}\n",
    "config['csvs_diretory'] = '/home/rafael/Área de Trabalho/Projetos/TextCategorizationToolPython/teste'\n",
    "config['output_directory'] = '/home/rafael/Área de Trabalho/Projetos/TextCategorizationToolPython/saida'\n",
    "config['text_column'] = 'letras'\n",
    "config['class_column'] = 'genero'\n",
    "config['pre-processing'] = True\n",
    "config['pre-processing_steps'] = {'language' : 'en', 'remove_stopwords' : True, 'remove_punctuation' : True, \n",
    "                 'convert_numbers' : True, 'remove_numbers' : True, 'simplification' : True, \n",
    "                 'simplification_type' : 'lemmatization', 'lower_case' : True}\n",
    "config['sparse_representation'] = {'use': False, 'n-grams' : [1], 'term-weights' : ['tf', 'tf-idf']}\n",
    "config['low-dimension_representation'] = {'use' : True, 'types' : ['doc2vec', 'word2vec'] ,\n",
    "                                          'doc2vec_config' : {'use' : False, 'models' : ['dm', 'dbow', 'both'], 'methods' : ['average','concat'], \n",
    "                                          'num_threads': 4, 'alpha' : 0.025, 'min_alpha' : 0.0001,\n",
    "                                          'num_max_epochs' : [1, 3, 100, 1000], 'min_count' : 1, 'window_sizes' : [5, 8, 10], \n",
    "                                          'num_dimensions' : [25,  50, 100, 500, 1000] }, \n",
    "                                          'word2vec_config' : {'use' : True, 'models' : ['sg','cbow','fasttext'], 'methods' : ['average','sum'], \n",
    "                                          'num_threads': 4, 'alpha' : 0.025, 'min_alpha' : 0.0001,                       \n",
    "                                          'num_max_epochs' : [1, 3, 5, 50, 100], 'min_count' : 5, 'window_sizes' : [5, 8, 10], \n",
    "                                          'num_dimensions' : [25,  50, 100, 300] }\n",
    "                                         }\n",
    "config['save-arff'] = False\n",
    "config['save-binary'] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Json\n",
    "def save_json(path_json): \n",
    "    with open(path_json, 'w') as outfile:\n",
    "        json.dump(config, outfile, indent=4, ensure_ascii=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Json\n",
    "def load_json(path_json): \n",
    "    with open(path_json) as json_file:\n",
    "        return json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(path, text_column, class_column): \n",
    "    df = pd.read_csv(path)\n",
    "    df = df.dropna()\n",
    "    texts = df[config['text_column']]\n",
    "    classes = df[config['class_column']]\n",
    "    return texts, classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_name(name, representation_type, config): \n",
    "    final_name = f'{name}_{representation_type}'\n",
    "    for item in config.items(): \n",
    "        final_name += f'_{item[0]}={item[1]}'\n",
    "    \n",
    "    return final_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_save_representation(config, rep_builder, texts, classes, name_builder ,parameters, dataset_name, non_sparse_format): \n",
    "    representation_name = build_name(dataset_name, name_builder, parameters)\n",
    "    path_out_arff = os.path.join(config['output_directory'], representation_name + '.arff') \n",
    "    path_out_bin = os.path.join(config['output_directory'], representation_name + '.rep') \n",
    "\n",
    "    if os.path.exists(path_out_arff) or os.path.exists(path_out_bin): \n",
    "        return \n",
    "\n",
    "    representation = rep_builder.build_representation(texts,classes)\n",
    "\n",
    "    if config['save-arff'] == True: \n",
    "        representation.save_arff(representation_name, path_out_arff, non_sparse_format = non_sparse_format)\n",
    "    if config['save-binary'] == True: \n",
    "        save_representation(representation, path_out_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_all(config): \n",
    "\n",
    "    #Getting the directory of the csvs and listing the csvs \n",
    "    text_preprocessor = TextPreprocessor(**config['pre-processing_steps'])\n",
    "    directory = config['csvs_diretory']\n",
    "    for csv_file in sorted(os.listdir(directory)):\n",
    "        dataset_name = csv_file[:csv_file.rindex('.')]\n",
    "        print('=============================================')\n",
    "        print('=============================================')\n",
    "        print('Dataset: ', dataset_name)\n",
    "        \n",
    "        # Loading the CSVs and getting the column of the texts and the classes\n",
    "        texts, classes = load_csv(os.path.join(directory,csv_file), config['text_column'], config['class_column'])\n",
    "\n",
    "        #Pre-prossing texts\n",
    "        if config['pre-processing'] == True: \n",
    "            print('Preprocessing text collection')\n",
    "            texts = texts.apply(text_preprocessor.text_cleaner)\n",
    "        \n",
    "        #Processing sparse representations\n",
    "        if config['sparse_representation']['use'] == True: \n",
    "            print('=============================================')\n",
    "            print('Sparse Representation')\n",
    "            for ngram in config['sparse_representation']['n-grams']: \n",
    "                print('N-gram: ', ngram)\n",
    "                for term_weight in config['sparse_representation']['term-weights']: \n",
    "                    print('Term-weight: ', term_weight)\n",
    "                    parameters = {'term-weight' : term_weight, 'n-grams' : ngram}\n",
    "                    mySparseVSM = MySparseVSM(weight=term_weight, n_grams=ngram)\n",
    "                    build_and_save_representation(config, mySparseVSM, texts, classes, 'SparseVSM' ,parameters, dataset_name, True)\n",
    "                \n",
    "        #Processing low-dimensional representations\n",
    "        if config['low-dimension_representation']['use'] == True:\n",
    "            for type_repr in config['low-dimension_representation']['types']: \n",
    "                if type_repr == 'doc2vec': \n",
    "                    if config['low-dimension_representation']['doc2vec_config']['use'] == True: \n",
    "                        print('=============================================')\n",
    "                        print('Doc2Vec')\n",
    "                        for model in config['low-dimension_representation']['doc2vec_config']['models']:\n",
    "                            print('Model:', model)\n",
    "                            for method in config['low-dimension_representation']['doc2vec_config']['methods']: \n",
    "                                print('Method:', method)\n",
    "                                for num_max_epoch in config['low-dimension_representation']['doc2vec_config']['num_max_epochs']:\n",
    "                                    print('Num. Max Epochs:', num_max_epoch)\n",
    "                                    for window_size in config['low-dimension_representation']['doc2vec_config']['window_sizes']:\n",
    "                                        print('Window Size:', window_size)\n",
    "                                        for num_dimensions in config['low-dimension_representation']['doc2vec_config']['num_dimensions']:\n",
    "                                            print('Num. Dimensions:', num_dimensions)\n",
    "                                            parameters = {'model' : model, 'method' : method, 'dim_size': num_dimensions,\n",
    "                                                        'num_max_epochs' : num_max_epoch, 'window_size' : window_size, \n",
    "                                                        'num_threads' : config['low-dimension_representation']['doc2vec_config']['num_threads'],\n",
    "                                                        'min_count' : config['low-dimension_representation']['doc2vec_config']['min_count'],\n",
    "                                                        'alpha' : config['low-dimension_representation']['doc2vec_config']['alpha'],\n",
    "                                                        'min_alpha' : config['low-dimension_representation']['doc2vec_config']['min_alpha']\n",
    "                                                        }\n",
    "\n",
    "                                            myDoc2Vec = MyDoc2Vec(**parameters)\n",
    "                                            build_and_save_representation(config, myDoc2Vec, texts, classes, 'Doc2Vec', parameters, dataset_name, False)\n",
    "                elif type_repr == 'word2vec': \n",
    "                    if config['low-dimension_representation']['word2vec_config']['use'] == True:\n",
    "                        print('=============================================')\n",
    "                        print('Word2Vec')\n",
    "                        for model in config['low-dimension_representation']['word2vec_config']['models']:\n",
    "                            print('Model:', model)\n",
    "                            for method in config['low-dimension_representation']['word2vec_config']['methods']: \n",
    "                                print('Method:', method)\n",
    "                                for num_max_epoch in config['low-dimension_representation']['word2vec_config']['num_max_epochs']:\n",
    "                                    print('Num. Max Epochs:', num_max_epoch)\n",
    "                                    for window_size in config['low-dimension_representation']['word2vec_config']['window_sizes']:\n",
    "                                        print('Window Size:', window_size)\n",
    "                                        for num_dimensions in config['low-dimension_representation']['word2vec_config']['num_dimensions']:\n",
    "                                            print('Num. Dimensions:', num_dimensions)\n",
    "                                            parameters = {'model' : model, 'method' : method, 'dim_size': num_dimensions,\n",
    "                                                        'num_max_epochs' : num_max_epoch, 'window_size' : window_size, \n",
    "                                                        'num_threads' : config['low-dimension_representation']['word2vec_config']['num_threads'],\n",
    "                                                        'min_count' : config['low-dimension_representation']['word2vec_config']['min_count'],\n",
    "                                                        'alpha' : config['low-dimension_representation']['word2vec_config']['alpha'],\n",
    "                                                        'min_alpha' : config['low-dimension_representation']['word2vec_config']['min_alpha']\n",
    "                                                        }\n",
    "\n",
    "                                            myWord2Vec = MyWord2Vec(**parameters)\n",
    "                                            build_and_save_representation(config, myWord2Vec, texts, classes, 'Word2Vec', parameters, dataset_name, False)\n",
    "                    pass\n",
    "                else: \n",
    "                    raise ValueError('Unsuported low dimension representation type. Please, use types = {\"doc2vec\",\"word2vec\"}.')\n",
    "                \n",
    "                \n",
    "\n",
    "    print('Process Concluded!!')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "=============================================\n",
      "Dataset:  pop_sub\n",
      "Preprocessing text collection\n",
      "=============================================\n",
      "Word2Vec\n",
      "Model: sg\n",
      "Method: average\n",
      "Num. Max Epochs: 1\n",
      "Window Size: 5\n",
      "Num. Dimensions: 25\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'unknown' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f4ee6cd2510b>\u001b[0m in \u001b[0;36msum_vectors\u001b[0;34m(self, lista_tokens)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mvetor_combinado\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'friday' not in vocabulary\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c91c3a1445ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocess_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-f4467e2ff38e>\u001b[0m in \u001b[0;36mprocess_all\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                                             \u001b[0mmyWord2Vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                                             \u001b[0mbuild_and_save_representation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyWord2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Word2Vec'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-14e97c963e02>\u001b[0m in \u001b[0;36mbuild_and_save_representation\u001b[0;34m(config, rep_builder, texts, classes, name_builder, parameters, dataset_name, non_sparse_format)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mrepresentation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrep_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_representation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'save-arff'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f4ee6cd2510b>\u001b[0m in \u001b[0;36mbuild_representation\u001b[0;34m(self, texts, classes)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'average'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-f4ee6cd2510b>\u001b[0m in \u001b[0;36msum_vectors\u001b[0;34m(self, lista_tokens)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                     \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mvetor_combinado\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvetor_combinado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/textmining/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'unknown' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "process_all(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__': \n",
    "    path_json = sys.argv[1]\n",
    "    if not os.path.exists(path_json): \n",
    "        print('Incorrect path for JSON file')\n",
    "        sys.exit(0)\n",
    "    else: \n",
    "        extension = os.path.splitext(path_json)[1]\n",
    "        if extension.lower() != 'json': \n",
    "            print('Invalid extension file')\n",
    "            sys.exit(0)\n",
    "        else: \n",
    "            config = load_json(path_json)\n",
    "            process_all(config)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
